# Evaluation Strategy and Metrics

Evaluating the models involves assessing both their ability to predict the continuous `award_share` value and their effectiveness in ranking players correctly based on those predictions.

## 1. Regression Metrics

Standard regression metrics are calculated on the test set to evaluate the accuracy of the predicted `award_share`:

*   **Mean Squared Error (MSE):** `sklearn.metrics.mean_squared_error(y_test, y_pred)`
*   **Mean Absolute Error (MAE):** `sklearn.metrics.mean_absolute_error(y_test, y_pred)`
*   **R-squared (RÂ²):** `sklearn.metrics.r2_score(y_test, y_pred)`

These metrics are often calculated both for the entire test set and separately for only the players who actually received votes (`award_share > 0`) to understand performance on the most relevant subset of players.

## 2. Ranking Metrics

Since the ultimate goal involves ranking, specific ranking metrics are crucial.

### 2.1. Custom Rank Accuracy (Partial Credit)

*   **Function:** `calculate_rank_accuracy(true_ranks, pred_ranks, max_rank=5)` (defined in `predict_mvp_rank.py`, `mlp_nn.py`, etc.)
*   **Process:**
    1.  Convert actual and predicted `award_share` values to ranks within each season using `convert_to_ranks` (higher share = better rank).
    2.  Focus only on players whose **true rank** is within the top `k` (e.g., `max_rank=5`).
    3.  Calculate the absolute difference between the predicted rank and the true rank for these top-k players.
    4.  Assign points based on the difference:
        *   Diff = 0: 1.0 points
        *   Diff = 1: 0.8 points
        *   Diff = 2: 0.6 points
        *   Diff = 3: 0.4 points
        *   Diff = 4: 0.2 points
        *   Diff > 4: 0.0 points
    5.  Sum the points awarded for all top-k players across all test seasons.
    6.  Calculate overall accuracy by dividing the total points by the total number of actual top-k players considered.
*   **Purpose:** Provides a nuanced view of ranking quality, giving partial credit for near-misses within the top ranks.

### 2.2. Normalized Discounted Cumulative Gain (NDCG@k)

*   **Function:** `calculate_ndcg(true_values, pred_values, k=5)` (defined in `_ndcg.py` variants)
*   **Implementation:** Uses `sklearn.metrics.ndcg_score`.
*   **Process:**
    1.  For each season, use the actual `award_share` as the true relevance score and the predicted `award_share` as the predicted score.
    2.  Calculate the NDCG score for the top `k` (e.g., 5) positions.
    3.  NDCG compares the predicted ranking order (based on predicted scores) to the ideal ranking order (based on true scores), considering the position of relevant items (players with higher `award_share`). It discounts the value of relevant items ranked lower.
    4.  Scores range from 0 to 1, where 1 indicates perfect ranking.
    5.  Average the NDCG@k scores across all test seasons.
*   **Purpose:** Standard, widely used metric for evaluating ranked lists, sensitive to the position of highly relevant items.

### 2.3. Top-k Actual vs. Predicted Comparison

*   **Function:** `save_top5_comparison(results_df, k=5)` (defined in `_ndcg.py` variants)
*   **Process:**
    1.  For each season, identify the top `k` players based on actual `award_share`.
    2.  Identify the top `k` players based on predicted `award_share`.
    3.  Create a table comparing these lists side-by-side for each rank position (1 to k).
    4.  Calculate the percentage of players correctly predicted at their exact rank within the top k.
*   **Purpose:** Provides a direct, interpretable comparison of the highest-ranked players.

## 3. Visualizations

Evaluation also relies heavily on visualizations generated by functions like `generate_evaluation_visualizations` or `create_prediction_visualizations` and `create_ranking_visualizations_ndcg`:

*   Actual vs. Predicted Award Share Scatter Plots
*   Prediction Error Distribution Histograms
*   Performance Metrics (MSE, Accuracy, NDCG) per Season Bar Charts
*   Rank Error Distribution Count Plots (for Custom Rank Accuracy)
*   Confusion Matrices for Top-k Ranks (for Custom Rank Accuracy)

## 4. Standalone Evaluation Script

The script `scripts/evaluate_award_share_rankings.py` seems designed to perform evaluations (potentially using the custom rank accuracy metric) on pre-existing prediction files, allowing for evaluation without retraining. 