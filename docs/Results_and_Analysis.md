# Results and Analysis

*This document summarizes the performance of the different models. Note that the specific results are generated by running the scripts and may vary based on the exact data split, random seeds, and environment.*

## 1. Overall Model Comparison

Summarize the key performance metrics (Overall Custom Rank Accuracy, Average NDCG@5, R², MAE, MSE) for each model category in a table.

| Model Category               | Custom Rank Accuracy (%) | Avg NDCG@5 | R² (All) | MAE (All) | MSE (All) | R² (Vote Getters) | MAE (Vote Getters) | MSE (Vote Getters) |
| :------------------------- | :-----------------------: | :--------: | :------: | :-------: | :-------: | :---------------: | :----------------: | :----------------: |
| Baseline ML (LinReg)       | *Result*                  |   0.580    | *Result* | *Result*  | *Result*  | *Result*          | *Result*           | *Result*           |
| Baseline ML (RF)           | *Result*                  |   0.681    | *Result* | *Result*  | *Result*  | *Result*          | *Result*           | *Result*           |
| Baseline ML (GB)           | *Result*                  |   0.678    | *Result* | *Result*  | *Result*  | *Result*          | *Result*           | *Result*           |
| Simple MLP                 | *Result*                  |    N/A     | *Result* | *Result*  | *Result*  | *Result*          | *Result*           | *Result*           |
| Advanced MLP               | *Result*                  |   0.805    | *Result* | *Result*  | *Result*  | *Result*          | *Result*           | *Result*           |
| Transformer                | *Result*                  |   0.768    | *Result* | *Result*  | *Result*  | *Result*          | *Result*           | *Result*           |
| Ensemble                   | *Result*                  |   0.751    | *Result* | *Result*  | *Result*  | *Result*          | *Result*           | *Result*           |

*(Populate the table with actual results from running the scripts, typically found in saved CSV files like `model_comparison_with_ndcg.csv` or printed output.)*

## 2. Analysis of Ranking Performance

*   **NDCG Trends:** Discuss how NDCG@5 varies across seasons for the best-performing models (referencing `ndcg_by_season.png`). Are there specific eras or seasons that are harder/easier to rank?
*   **Custom Accuracy Trends:** Similarly, discuss the trends in the custom rank accuracy metric across seasons (`rank_accuracy_by_season.png`).
*   **Top-k Comparison Insights:** Analyze the `top5_actual_vs_predicted.csv` files. How often do the models correctly identify the exact player at each rank (1-5)? Where do the biggest discrepancies occur?
*   **Error Analysis:** Examine the rank error distributions (`rank_error_distribution.png`) and confusion matrices (`rank_confusion_matrix.png`). Are models more likely to be off by 1 rank, or are there larger errors? Do they confuse specific ranks frequently (e.g., predicting rank 2 as rank 3)?

## 3. Analysis of Award Share Prediction

*   **Overall Fit:** Discuss the R² values. How much variance in `award_share` do the models explain?
*   **Prediction Errors:** Analyze the MAE and MSE. How large are the typical prediction errors in terms of award share points?
*   **Vote Getters vs. All Players:** Compare the performance metrics for all players versus only those who received votes. Do models struggle more with predicting zero vs. non-zero shares, or with predicting the magnitude of non-zero shares?
*   **Visual Insights:** Refer to the actual vs. predicted scatter plots (`award_share_actual_vs_predicted.png`, `award_share_vote_getters_actual_vs_predicted.png`). Do the plots show systematic biases (e.g., underpredicting high award shares)?

## 4. Model Architecture Comparison

*   **Complexity vs. Performance:** Did the more complex architectures (Transformer, Ensemble, Advanced MLP) significantly outperformed the simpler ones (Baseline ML, Simple MLP) after accounting for potential overfitting?
*   **Feature Engineering Impact:** How important was the feature selection step? (Referencing correlations or PCA results if available).
*   **Strengths/Weaknesses:** Briefly discuss the perceived advantages or disadvantages of each approach based on the results.

## 5. Conclusion

Summarize the key findings. Which model(s) performed best overall for the ranking task? What are the main limitations and potential areas for future improvement? 